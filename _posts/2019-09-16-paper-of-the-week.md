---
layout: post
tags: social
---
![Robust and non robust features](/images/63874675-95a69100-c987-11e9-96e4-d6d2c1e6c082.png){:width="300px"}
![Discussion of Adversarial Examples Are Not Bugs, They Are Features](/images/63874706-a6ef9d80-c987-11e9-92ee-0949c2572d06.png){:width="300px"}

### Paper of the Week: "Adversarial Examples Are Not Bugs, They Are Features"

This paper proposes that "adversarial examples are due to 'non-robust features' which are highly predictive but imperceptible to humans". That is, adversarial picture examples aren't a sign that your original network has bugs -- these examples contain predictive features, they just aren't ones that are visible to us humans!
I thought this initial paper was interesting and enjoyed the very meta title, but I REALLY liked the follow up discussion and replication on distill.pub.


Read the paper [here](https://arxiv.org/pdf/1905.02175.pdf)

If you're not sure you're getting it, try reading some summaries [here](https://www.shortscience.org/paper?bibtexKey=ilyas2019adversarial)
And catch the follow up discussion [here](https://distill.pub/2019/advex-bugs-discussion/)
